内存管理机制

Linux的内存管理采取的是分页存取机制：为了保证物 理内存能得到充分的利用，内核会在适当的时候将物理内存中不经常使用的数据块自动交换到虚拟内存中，而将经常使用的信息保留到物理内存。

buffers与cached
buffers与cached都是内存操作，用来保存系统曾经打开过的文件以及文件属性信息，这样当操作系统需要读取某些文件时，会首先在buffers 与cached内存区查找，如果找到，直接读出传送给应用程序，如果没有找到需要数据，才从磁盘读取，这就是操作系统的缓存机制，通过缓存，大大提高了操 作系统的性能。但buffers与cached缓冲的内容却是不同的。

buffers是用来缓冲块设备做的，它只记录文件系统的元数据（metadata）以及 tracking in-flight pages，而cached是用来给文件做缓冲。更通俗一点说：buffers主要用来存放目录里面有什么内容，文件的属性以及权限等等。而cached直接用来记忆我们打开过的文件和程序。

内存释放
linux系统中/proc是一个虚拟文件系统，我们可以通过对它的读写操作做为与kernel实体间进行通信的一种手段。也就是说可以通过修改 /proc中的文件，来对当前kernel的行为做出调整。那么我们可以通过调整/proc/sys/vm/drop_caches来释放内存。

echo 10 > /proc/sys/vm/dirty_background_rato (当脏数据占据物理内存10%时，触发pdflush同步到硬盘）
echo 2000 > /proc/sys/vm/dirty_expire_centisecs (当脏数据在物理内存的逗留时间超过2000ms时被同步到硬盘）
echo 10 > /proc/sys/vm/swappiness(值为0表示尽量都用物理内存，值为100表示积极的使用swap分区；）

cpu利用率和cpu 队列
CPU时间片
在Linux的内核处理过程中，每一个进程默认会有一个固定的时间片来执行命令（默认为1/100秒），这段时间内进程被分配到CPU，然后独占使用。如果使用完，同时未到时间片的规定时间，那么就主动放弃CPU的占用，如果到时间片尚未完成工作，那么CPU的使用权也会被收回，进程将会被中断挂起等待下一个时间片。
CPU利用率= CPU使用时间的总和去除以统计的时间段
Average Load体现的是在某一统计时间段内，所有使用电话的人加上等待电话分配的人一个平均统计

echo 03 > /proc/irq/19/smp-affinity (将中断类型为19的中断绑定到第三个cpu上处理）

I/O:
根据应用类型，适当调整page size 和block size;
调整进程I/O请求的优先级，分三种级别：1代表 real time ; 2代表best-effort; 3代表idle

网络可调性能参数 :
1.查看网卡设置是否全双工传输的： echtool eth0
2. 设置MTU（最大传输单元），在带宽G以上的时候，要考虑将MTU增大，提高传输性能；如： ifconfig eth0 mtu 9000 up
3. 增加网络数据缓存；传输数据时linux是将包先放入缓存，填满缓存后即发送出去；读操作类似；
sysctl -w net.ipv4.tcp_rmem="4096 87380 8388608" :设置tcp读缓存：最小缓存，初始化时，最大缓存
sysctl -w net.ipv4.tcp_wmem="4096 87380 8388608" ：设置tcp写缓存：最小缓存，初始化时，最大缓存
4.禁用window_scaling,并且直接设置window_size;(就像我们经常设置jvm的参数：xms = xmx一样
sysctl -w net.ipv4.tcp_window_scaling=0
5.设置TCP连接可重用性： 对于TIME_OUT状态的TCP连接可用于下一个TCP重用，这样减少了三次握手和创建时间，非常提高性能，尤其对于web server；
如： 开启可重用tcp功能： sysctl -w net.ipv4.tcp_tw_reuse=1 sysctl -w net.ipv4.tcp_tw_recyle=1 
6.禁用掉没必要的tcp/ip协议功能：比如icmp;broadcast包的接收；
7. linux对于keeplive的tcp连接有一个默认的过期时间；可以减小这个时间，让没用的连接释放掉，毕竟tcp连接数是有限的嘛(连接数受端口限制1024～65535,且受系统打开文件数量限制ulimit)；
如： sysctl -w net.ipv4.tcp_keepalive_time=1800 （设置过期时间，1800s)
8.设置最大tcp正在连接状态（还没ESTABLISHED)队列长度；避免由于太多的tcp连接过来，导致服务器挂掉；比如DoS攻击
如：sysctl -w net.ipv4.tcp_max_syn_backlog=4096
9. 绑定tcp类型的中断到一个cpu上；（让cpu去亲和这个类型中断，避免频繁的中断，影响线程调度性能）

lsof  -i:port  可以检测到打开套接字的状况
sar -n SOCK 查看tcp创建的连接数
tcpdump -iany tcp port 9000 对tcp端口为9000的进行抓包
netstat -nat  查看TCP各个状态的数量
 awk  '{ip[$1]++} END{for(i in ip) {print i,ip[i]}}' /var/log/httpd/access_log		apache访问ip统计
netstat -n|awk '/^tcp/{++S[$NF]}END{for (key in S) print key,S[key]}'
TIME_WAIT 286		另一边已初始化一个释放
FIN_WAIT1 5			应用说它已经完成
FIN_WAIT2 6			另一边已同意释放
ESTABLISHED 269		正常数据传输状态
SYN_RECV 5			一个连接请求已经到达，等待确认
CLOSING 1			两边同时尝试关闭

如发现系统存在大量TIME_WAIT状态的连接，通过调整内核参数解决：
编辑文件/etc/sysctl.conf，加入以下内容：
net.ipv4.tcp_syncookies = 1		表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
net.ipv4.tcp_tw_reuse = 1		允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
net.ipv4.tcp_tw_recycle = 1		表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭
net.ipv4.tcp_fin_timeout = 30

开启路由转发
echo "1" > /proc/sys/net/ipv4/ip_forward
echo "net.ipv4.ip_forward = 1" > /sysctl.conf
sysctl -p
============================================================================
iptalbles
4表：
filter--->nat--->mangle--->raw
5链：
Input--->output--->forward--->prerouting--->postrouting

iptables -A INPUT -p tcp --dport 22 -j ACCEPT
1、iptables命令格式
     iptables [-t 表] －命令 匹配 操作 （大小写敏感）
   动作选项
     ACCEPT          接收数据包
     DROP             丢弃数据包
     REDIRECT      将数据包重新转向到本机或另一台主机的某一个端口，通常功能实现透明代理或对外开放内网的某些服务
     SNAT             源地址转换
     DNAT             目的地址转换
     MASQUERADE       IP伪装
     LOG               日志功能
2、定义规则
   ①先拒绝所有的数据包，然后再允许需要的数据包
      iptalbes -P INPUT DROP
      iptables -P FORWARD DROP
      iptables -P OUTPUT ACCEPT
   ②查看nat表所有链的规则列表
      iptables -t nat -L
   ③增加，插入，删除和替换规则
     iptables [-t 表名] <-A|I|D|R> 链名 [规则编号] [-i|o 网卡名称] [-p 协议类型] [-s 源ip|源子网] [--sport 源端口号] [-d 目的IP|目标子网] [--dport 目标端口号] [-j 动作]
    参数：-A 增加
               -I 插入
               -D 删除
               -R 替换
例子
①iptables -t filter -A INPUT -s 192.168.1.5 -i eth0 -j DROP
禁止IP为192.168.1.5的主机从eth0访问本机
②iptables -t filter -I INPUT 2 -s 192.168.5.0/24 -p tcp --dport 80 -j DROP
禁止子网192.168.5.0访问web服务
③iptables -t filter -I INPUT 2 -s 192.168.7.9 -p tcp --dport ftp -j DROP
禁止IP为192.168.7.9访问FTP服务
④iptables -t filter -L INPUT
查看filter表中INPUT链的规则
⑤iptables -t nat -F
删除nat表中的所有规则
⑥iptables -I FORWARD -d wwww.playboy.com -j DROP
禁止访问www.playboy.com网站
⑦iptables -I FORWARD -s 192.168.5.23 -j DROP
禁止192.168.5.23上网
SNAT地址转换:
 iptables  -t  nat  -A POSTROUTING  -s  192.168.4.0/24 -p tcp --dport 80  -j SNAT  --to-source 192.168.2.5
所有iptables规则都是临时规则，如果需要永久保留规则需要执行如下命令:
service  iptables save
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Apache的三种工作模式及相关配置：
1、Prefork MPM--->多进程        
优点：成熟，兼容所有新老模块    
缺点：一个进程相对占用更多的系统资源，消耗更多的内存。
2、Worker MPM--->多进程+多线程
优点：占据更少的内存，高并发下表现更优秀。
缺点：必须考虑线程安全的问题，因为多个子线程是共享父进程的内存地址的。
3、Event MPM--->多进程+多线程+epoll
Apache中最新的模式，在现在版本里的已经是稳定可用的模式。它和 worker模式很像，最大的区别在于，它解决了 keep-alive 场景下 ，长期被占用的线程的资源浪费问题.event MPM需要Linux系统（Linux 2.6+）对Epoll的支持，才能启用。

HTTP协议的主要特点可概括:
1.支持客户/服务器模式。
2.简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。
3.灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。
4.无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。
5.无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。

http请求由三部分组成，分别是：请求行、消息报头、请求正文
HTTP响应也是由三个部分组成，分别是：状态行、消息报头、响应正文
-----------------------------------------------------------------------------------------------------------------------------------------------------------
linux系统调优：
控制进程资源的使用
/proc/2822（对应pid）/io：包括该进程的 IO 统计信息（IO 操作时的读写字符数）
cat /boot/config-$(uname -r) | grep -i cgroups  查看是否开启进程所属的控制组
CONFIG_CGROUPS=y	表示开启
Linux 中为每个用户限制的进程数(防止死循环 fork bomb)
/etc/security/limits.conf 文件末尾添加下面一行来设置限制：*  hard  nproc 10
Linux 进程管理工具:
renice 调整执行优先级（系统资源的使用）
 通过使用 renice 调整执行优先级（系统资源的使用）。这意味着内核会根据分配的优先级（众所周知的 “niceness”，它是一个范围从-20到19的整数）给进程分配更多或更少的系统资源。这个值越小，执行优先级越高
语法：renice [-n] <new priority> <UID, GID, PGID, or empty> identifier
     renice <优先级 > <进程ID>
如果 new priority 后面的参数没有（为空），默认就是 PID。在这种情况下，PID=identifier 的进程的 niceness 值会被设置为<new priority>
Linux 进程和资源监视常用基本命令:
ps	top	 lsof	strace	ltrace	time  renice  uptime  free	vmstat  iostat
------------------------------------------------------------------------------------------------------------------------------------------------------------
Mysql问题及解决：
1.错误日志有类似警告：
120119 16:26:04 [Warning] IP address '192.168.1.10' could not be resolved: Name or service not known
解决:修改配置文件添加并需要重启  
[mysqld] 
skip-name-resolve
2.问题错误日志：Error: Can’t create a new thread (errno 12)
数据库服务器问题，数据库操作无法创建新线程。一般是有以下3个方面的原因： 
1）、MySQL 线程开得太多。 
2）、服务器系统内存溢出。 
3）、环境软件损坏或系统损坏。
【问题解决】
1）进入 phpmyadmin 的 mysql 数据库中的 user 表，对数据库的用户进行编辑，修改 max_connections 的值。适当的改小一点。 
2）联系服务器管理员检查服务器的内存和系统是否正常，如果服务器内存紧张，请检查一下哪些进程消耗了服务器的内存，同时考虑是否增加服务器的内存来提高整个系统的负载能力。 
3）mysql版本更改为稳定版本 
4）优化网站程序的sql等等
3. 操作报错：删除库时:ERROR 1010 (HY000): Error dropping database
原因：在该库目录内含有自己放的文件，删除自己放的文件后再操作即可
4.导出数据很快，导入到新库时却很慢：
在导出时合理使用几个参数，可以大大加快导入的速度。
-e 使用包括几个VALUES列表的多行INSERT语法; 
--max_allowed_packet=XXX 客户端/服务器之间通信的缓存区的最大大小; 
--net_buffer_length=XXX TCP/IP和套接字通信缓冲区大小,创建长度达net_buffer_length的行 
注意：max_allowed_packet和net_buffer_length不能比目标数据库的配置数值大，否则可能出错。
-------------------------------------------------------------------------------------------------------------------------------------------------------
LVS模式：
LVS常见的问题及原因分析
一、裂脑：
由于两台高可用服务器对之间，在指定时间内，无法相互检测到对方的心跳，而各自启动故障切换转移功能，取得资源服务及所有权，而此时的两台高可用服务器对都还或者，并且正在运行，这样就会导致同一个IP或服务在两段同时启动而发生冲突的严重问题，最严重的是两台主机占用同一个VIP，当用户写入数据的时候可能同时写在两台服务器上。 
1）产生裂脑原因：
1、心跳链路故障，导致无法通信----->心跳线坏了（故障或老化）
2、开启防火墙阻挡心跳消息传输----->网卡相关驱动坏了，IP配置即冲突问题（直连）
3、心跳网卡地址配置等不正确    ----->心跳线间连接的设备故障（网卡及交换机） 
4、其他：心跳方式不同，心跳广播冲突，软件bug等------>仲裁机器出问题
2）防止裂脑的方法:
1、采用串行或以太网电缆连接，同时用两条心跳线路
2、做好裂脑的监控报警，在问题发生时人为第一时间介入仲裁
3、启用磁盘锁，即正在服务的一方只在发现心跳线全部断开时，才开启磁盘锁
4、fence设备（智能电源管理设备）
5、增加仲裁盘
6、加冗余线路
二、lvs负载不均的原因
①lvs自身的会话保持参数设置。优化：使用cookie代替session
②lvs调度算法设置，例如rr、wrr、
③后端RS节点的会话保持参数，例如apache的keepalive参数
④访问量较少的情况下，不均衡的现象更加明显
⑤用户发送的请求时间长短和请求资源多少以及大小
-------------------------------------------------------------------------------------------------------------------------------------------------------
负载均衡器对比(LVS、Nginx、HAproxy)
衡量负载均衡器好坏的几个重要的因素：
1. 会话率 ：单位时间内的处理的请求数
2. 会话并发能力：并发处理能力
3. 数据率：处理数据能力
LVS:
1. 抗负载能力强，性能高,对内存和CPU资源消耗比较低
2.工作在网络4层，通过VRRP协议(仅作代理之用),具体的流量是由linux内核来处理，不产生流量。
3.稳定，可靠性高，自身有完美的热备方案(Keepalived+lvs)
4. 不支持正则处理，不能做动静分离。
5. 支持多种负载均衡算法：rr(轮询)，wrr(带权轮询)、lc(最小连接)、wlc(带权最小连接)
6.LVS工作模式有4种：nat 地址转换，dr 直接路由，tun 隧道，full-nat 
Nginx:
1. 工作在网络7层，可以针对http应用做一些分流的策略，比如针对域名，目录结构
2.Nginx对网络的依赖较小，理论上能ping通就能进行负载功能
3.ginx安装配置比较简单，测试起来很方便
4. 也可以承担较高的负载压力且稳定，nginx是为解决c10k问题而诞生的
5. 对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测
6.Nginx对请求的异步处理可以帮助节点服务器减轻负载压力
7.Nginx仅能支持http、https和Email协议，这样就在适用范围较小。
8. 不支持Session的直接保持，但能通过ip_hash来解决。
9. Nginx还能做Web服务器即Cache功能。
HAProxy:
1. 支持两种代理模式：TCP（四层）和HTTP（七层），支持虚拟主机；
2. 能够补充Nginx的一些缺点比如Session的保持，Cookie的引导等工作
3. 支持url检测后端的服务器出问题的检测会有很好的帮助。
4. 更多的负载均衡策略比如：动态加权轮循，加权源地址哈希，加权URL哈希和加权参数哈希已经实现
5. 单纯从效率上来讲HAProxy更会比Nginx有更出色的负载均衡速度。
6. HAProxy可以对Mysql进行负载均衡，对后端的DB节点进行检测和负载均衡。
7. 支持负载均衡算法：轮循、加权轮循、原地址保持、请求URL、根据cookie
8. 不能做Web服务器即Cache。
--------------------------------------------------------------------------------------------------------------------------------------------------------
yum -y install ansible      #网络yum
ansible.cfg 的查找顺序是 
1  ANSIBLE_CONFIG 变量定义的配置文件
2 当前目录下的 ./ansible.cfg 文件
3 前用户家目录下 ~/ansible.cfg 文件
4 /etc/ansible/ansible.cfg 文件

ansible  命令基础
ansible  主机分组  -m 模块  -a '命令和参数'

创建密钥对 id_rsa 是私钥，  id_rsa.pub 是公钥
ssh-keygen -t rsa -b 2048 -N ''

给所有主机部署密钥
ansible all -m authorized_key -a "user=root exclusive=true manage_dir=true key='$(< /root/.ssh/id_rsa.pub)'" -k

ansible-playbook --syntax-check yaml文件名        检查yaml文件语法
---
- name: Set authorized key took from file
  hosts: all
  tasks:
  - name:
  	authorized_key:
      user: charlie
      state: present
      key: "{{ lookup('file', '/root/.ssh/id_rsa.pub') }}"
ansible-playbook --syntak-check playbook.yaml   检测语法
ansible-playbook -C playbook.yaml				运行测试
ansible-playbook -C playbook.yaml --list		显示受到影响的主机
-------------------------------------------------------------------------------------------------------------------------------------------------------
kvm
kvm:是用来虚拟化或者说模拟CPU、内存等硬件的
	VMM:负责管理虚拟机的资源，并拥有所有虚拟机资源的控制权
	VMX:为了CPU层面支持VT技术,扩展了传统的x86处理器架构,引入了VMX模式，VMX分为root和non-root。
QEMU:只是用来虚拟化硬盘的
libvirt:提供了整个虚拟机的管理

openstack组件
Horizon--->提供web管理界面
Keystone-->提供认证身份管理服务
Neutron--->创建虚拟网络，提供网络服务
Cinder--->提供虚拟机的存储卷服务，为Nova中实例永久存储
Nova---->管理虚拟机服务
Glance---->虚拟机镜像注册角色
Swift----->提供对象块存储

--------------------------------------------------------------------------------------------------------------------------------------------------------
docker 自定义镜像仓库
1.vim /etc/docker/daemon.json
{"insecure-registries":["server IP:5000"]}
2.restart docker
3.docker run -d -p 5000:5000 registry
4.docker tag 镜像名:标签  serverIP:5000/镜像名:标签        打标签
5.docker run -it serverIP:5000/镜像名:标签           	    启动远程镜像
docker概念
依赖于iptables实现映射
容器的存储与端口映射
docker run -d      -p    5000:5000  镜像名:标签
存储卷的映射		                 宿主机:容器(容器内目录若存在则覆盖内容，不存在则创建目录)
docker run -d -v /var/webroot:/var/www/html   myos:httpd

查询镜像仓库有哪些镜像
curl http://serverIP:5000/v2/_catalog
curl http://serverIP:5000/v2/镜像名/tags/list
查看docker虚拟交换机信息
docker network list 
创建网络
docker network create --subnet 192.168.200.0/24 -d bridge docker1
docker network create --subnet 192.168.200.0/24 -d bridge -o com.docker.network.bridge.name=docker1 docker1
在新的网桥上创建容器
docker  run -it --network=br0  myos

docker insppect -f '{{.NetworkSetting.IPAddress}}' 镜像ID  
输出：172.17.0.2		查看/获取某个字段的值
-------------------------------------------------------------------------------------------------------------------------------------------------------
Kubernetes
1.master:集群的管理节点，负责管理集群，提供集群的资源数据访问入口。
Kubernetes API server提供HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口。也是集群控制的入口进程；Kubernetes Controller Manager是Kubernetes所有资源对象的自动化控制中心；Kubernetes Schedule是负责资源调度（Pod调度）的进程
2.node:集群架构中运行Pod的服务节点（亦叫agent或minion）。Node是Kubernetes集群操作的单元，用来承载被分配Pod的运行，是Pod运行的宿主机。关联Master管理节点，拥有名称和IP、系统资源信息。运行docker eninge服务，守护进程kunelet及负载均衡器kube-proxy.
3.Pod:运行于Node节点上，若干相关容器的组合。运行在同一宿主机上，使用相同的网络命名空间、IP地址和端口，能够通过localhost进行通。是Kurbernetes进行创建、调度和管理的最小单位
4.Replication Controller:用来管理Pod的副本，保证集群中存在指定数量的Pod副本。
5.Service:定义了Pod的逻辑集合和访问该集合的策略，是真实服务的抽象
Kubernetes的三种IP:
Node IP：是Kubernetes集群中节点的物理网卡IP地址，所有属于这个网络的服务器之间都能通过这个网络直接通信。这也表明Kubernetes集群之外的节点访问Kubernetes集群之内的某个节点或者TCP/IP服务的时候，必须通过Node IP进行通信
Pod IP：是每个Pod的IP地址，他是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络。
Cluster IP：是一个虚拟的IP,仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配P地址;无法被ping，他没有一个“实体网络对象”来响应;只能结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备通信的基础，并且他们属于Kubernetes集群这样一个封闭的空间。
-------------------------------------------------------------------------------------------------------------------------------------------------------
ELK(java)
elasticsearch  --  数据库、负责日志的存储  
logstash       --  收集日志、标准化的程序         input-->filter-->output
kibana         --  图形的展示工具

IP:9200/_cluster/health?pretty     查看集群状态
集群插件
/usr/share/elasticsearch/bin
下载插件文件到本地，必须使用 file:// 绝对路径安装
./plugin install file:///usr/share/elasticsearch/bin/bigdesk-master.zip 
使用远程 uri 路径可以直接安装
./plugin install ftp://192.168.1.254/elk/elasticsearch-head-master.zip
./plugin install ftp://192.168.1.254/elk/elasticsearch-kopf-master.zip
使用 ./plugin list 查看插件
集群 api 查询地址
http://192.168.1.15:9200/_cat
http://192.168.1.15:9200/_cat/nodes?v  显示详细信息
http://192.168.1.15:9200/_cat/nodes?help  显示帮助信息
创建索引
curl -XPUT 'http://192.168.1.13:9200/t1/' -d '{
    "settings":{
        "index":{
            "number_of_shards": 5,
            "number_of_replicas": 1
        }
    }
}'
_bulk  导入关键字
curl -X "POST" "http://192.168.1.13:9200/_bulk" --data-binary @shakespeare.json
如果没有 index 和 type ，我们需要自己指定一下 index 和 type
curl -X "POST" "http://192.168.1.13:9200/haha/xixi/_bulk" --data-binary @accounts.json
25444 夏











